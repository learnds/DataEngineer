# Sparkify Cloud Data Warehouse

## Summary

This is the Sparkify Data Warehouse built in the Cloud using Amazon AWS Redshift database. It's main goal is to support the Analytics team in analyzing user song listening preferences. It contains a star schema with one fact table called songplays and four dimension tables called user, artist, song and time.  Data for loading the tables is available in Amazon S3 - song data and log data. Song data contains song and artist information, while log data contains the log of user activity as they listen to various songs. The song data is used load song and artist dimensions and log data is used to load user and time dimensions as well as the fact table songplays.

### DataSet Description  

#### Song Dataset  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song

#### Log Dataset  
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### Schema Design
The Dimension tables user,artist and time are frequently used in analytic queries and are fairly small. So they are created using the distribution style 'all' indicating a copy of these tables is stored on each of the Redshift cluster nodes. This reduces data transfer across nodes during querying. All these 3 tables are stored in sorted order by their primary id column. The song dimension table is relatively large and hence distributed across the nodes by the song_id column in the sorted order of the song_id column.

The songplays fact table is quite large and so distributed by the song_id column and sorted by the start_time date column. This enables quick response times for time related queries.

### ETL Pipeline
The data stored in Json files in S3 is first loaded into staging tables to improve overall load times. The staging tables are called staging_events and stagings_songs. The dimension and fact tables are loaded by inserting data from these staging tables thus avoiding repeated reads of the JSON files which tends to be slower.

### Preliminary Data Analysis
There are 14896 rows loaded into staging_songs table. 8056 rows are loaded into staging_events table. 

The DW tables contain the following number of rows:
1. song -  14896
2. user - 97
3. artist - 9553
4. time - 8023
5. songplays - 769



### 
### Runtime instructions

The Data warehouse can be created using the create_tables.py script. To run the script, first create a Redshift cluster, update the dwh.cfg config file with connection parameters and run the following command:  

python create_tables.py  

For loading data into the fact and dimension tables, run the following script:  

python etl.py

### Files included in the project

README.md - this markup file  

create_tables.py - python script that connects to the Redshift database, creates staging, dimension and fact tables in Redshit database   

sql_queries.py - Python declaration script that contains DDL, Copy and  insert statements used in the project  

test_etl.ipynb - test python jupyter notebook for testing table creation, copy and insert statements

etl.py - Python script that runs the ETL to load the tables in the sparkifydb database  
