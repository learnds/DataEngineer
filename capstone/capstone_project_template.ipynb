{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project aims to build a cloud based datawarehouse in Redshift for analysing International visitor statistics by the US National Tourism and Trade Office. They would like to understand the distribution of visitors around all the airports in the US and the seasonal change in number of vistors. This would enable them to assign approriate number of personnel so visitor entry is efficiently handled. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd  \n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read AWS config to get access id\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('dw.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "The US Tourism and Trade office would like to understand the following visitor statistics using the I-94 vistor arrival data.\n",
    "\n",
    "1. Visitor arrival by month and year\n",
    "\n",
    "2. Visitors by arrival City and by date\n",
    "\n",
    "3. Visitors by Country and date\n",
    "\n",
    "The proposed solution is to use AWS to stage and load data. The use of AWS cloud technology enables linear scalabity as the data set grows. AWS also supports a variety of techonologies such as S3, Reshift, EMR and Cassandra suitable for handling massive amounts of data and also to proivde fast response times to end user queries. \n",
    "\n",
    "Initial data analysis is done by using Py Spark in the local \n",
    "\n",
    "The data will be loaded into fact and dimension tables in an Amazon Redshift data warehouse hosted in the cloud. Data is first copied to Amazon S3 service. Using S3 service will enable to scale the data staging area in case huge gigabyte size files are required to be processed. Also, the storage can be scaled back once the processing is done to save cost.\n",
    "\n",
    "Data will be first loaded into staging tables in Amazon Redshift using Copy statements. Following this, data cleansing can be perfomed and data loaded into fact and dimension tables using insert statements. The same SQL queries can be used to create ETL pipelines if required at a later stage.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "The project uses the following Udacity supplied data sets to build the data warehouse.\n",
    "\n",
    "1. I94 immigration SAS data files\n",
    "   This data is provided by the US travel department and is available here - https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "2. Country codes data in csv data format created using I94_SAS_Labels_description dictionary\n",
    "3. Arrival port - City and State data in pipe delimited format created using I94_SAS_Labels_description dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "i94_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(i94_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.1\").\\\n",
    "config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']).\\\n",
    "config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']).\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count for ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat is  3157072\n",
      "Record count for ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat is  3096313\n"
     ]
    }
   ],
   "source": [
    "# Read i94 sas data for 2 months - Mar 2016 and Apr 2016\n",
    "sas_files=[\"../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\",\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\"]\n",
    "for sasfile in sas_files:\n",
    "    df_spark_i94 = spark.read.format('com.github.saurfang.sas.spark').load(sasfile)\n",
    "    print(f\"Record count for {sasfile} is \", df_spark_i94.count())\n",
    "    df_spark_i94.write.mode(\"append\").parquet(\"sas_parquet/\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6253385"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Read parquet data\n",
    "df_all_i94 = spark.read.parquet(\"sas_parquet/\")\n",
    "df_all_i94.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94 table count: 6253385\n"
     ]
    }
   ],
   "source": [
    "# Clean i94 data\n",
    "## Get distinct values for the columns selected - remove duplicates\n",
    "df_all_i94.createOrReplaceTempView(\"visitor_i94_table\")\n",
    "i94_table = spark.sql(\"select distinct cicid, i94yr, i94mon,i94cit,i94port,arrdate,i94mode,i94addr,depdate,i94bir,i94visa,visapost, \\\n",
    "gender,airline,fltno,visatype from visitor_i94_table\")\n",
    "print(\"i94 table count:\",i94_table.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|visapost|gender|airline|fltno|visatype|\n",
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "|  48.0|2016.0|   4.0| 101.0|    NYC|20545.0|    1.0|     NY|20572.0|  68.0|    2.0|     FLR|     M|     AA|00199|      B2|\n",
      "| 270.0|2016.0|   4.0| 103.0|    NYC|20545.0|    1.0|     NY|20560.0|  33.0|    2.0|    null|     M|     LH|00410|      WT|\n",
      "| 424.0|2016.0|   3.0| 103.0|    NYC|20514.0|    1.0|     NY|20574.0|  37.0|    2.0|     VNN|     M|     OS|00087|      B2|\n",
      "| 746.0|2016.0|   4.0| 103.0|    LOS|20545.0|    1.0|   null|20546.0|  12.0|    2.0|    null|  null|     AS|00291|      WT|\n",
      "|1324.0|2016.0|   3.0| 107.0|    LOS|20514.0|    1.0|     CA|20538.0|  33.0|    2.0|    null|  null|     LH|00456|      B2|\n",
      "|1372.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  28.0|    2.0|    null|     F|     SN|01401|      WT|\n",
      "|1392.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  16.0|    2.0|    null|     M|     AB|07450|      WT|\n",
      "|1422.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20551.0|  45.0|    2.0|    null|     M|     DL|00047|      WT|\n",
      "|1630.0|2016.0|   4.0| 104.0|    MIA|20545.0|    1.0|     FL|20553.0|  76.0|    2.0|    null|     M|     AB|07000|      WT|\n",
      "|1714.0|2016.0|   4.0| 104.0|    MIA|20545.0|    1.0|     FL|20561.0|  49.0|    2.0|    null|  null|     AA|00039|      WT|\n",
      "|1817.0|2016.0|   3.0| 108.0|    NEW|20514.0|    1.0|     DC|20516.0|  47.0|    1.0|    null|     M|     SK|00909|      WB|\n",
      "|1890.0|2016.0|   3.0| 108.0|    NEW|20514.0|    1.0|     NY|20527.0|  69.0|    2.0|    null|     F|     SK|00909|      WT|\n",
      "|1959.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20549.0|  49.0|    2.0|    null|     M|     LX|00022|      WT|\n",
      "|2414.0|2016.0|   3.0| 108.0|    WAS|20514.0|    1.0|     DC|20526.0|  69.0|    2.0|    null|     M|     FI|  645|      WT|\n",
      "|2640.0|2016.0|   3.0| 109.0|    MIA|20514.0|    1.0|     FL|20575.0|  28.0|    2.0|    null|     M|     AA|00039|      WT|\n",
      "|2845.0|2016.0|   4.0| 108.0|    ATL|20545.0|    1.0|     LA|20569.0|  57.0|    2.0|    null|     M|     DL|00071|      WT|\n",
      "|3103.0|2016.0|   4.0| 108.0|    NYC|20545.0|    1.0|     NY|20557.0|  79.0|    2.0|    null|     M|     BA|00113|      WT|\n",
      "|3304.0|2016.0|   4.0| 108.0|    MIA|20545.0|    1.0|   null|20548.0|  52.0|    2.0|    null|     F|     AA|01466|      WT|\n",
      "|3598.0|2016.0|   3.0| 111.0|    NEW|20514.0|    1.0|     FL|20517.0|  34.0|    1.0|    null|     M|     UA|00056|      WB|\n",
      "|3608.0|2016.0|   3.0| 111.0|    NEW|20514.0|    1.0|     NJ|20517.0|  43.0|    1.0|    null|     F|     LX|00018|      WB|\n",
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_table.show()\n",
    "#Write to AWS S3 \n",
    "i94_table.write.partitionBy(\"i94yr\", \"i94mon\").mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/i94visitors/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### df_spark_i94.show()\n",
    "#Write to AWS S3\n",
    "df_spark_i94.write.partitionBy(\"i94yr\", \"i94mon\").mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/i94data-parquet/i94_visitors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original airport count  55075\n",
      "Clean airport count  2019\n"
     ]
    }
   ],
   "source": [
    "#Clean airport data\n",
    "airport_df = spark.read.csv('./airport-codes_csv.csv', header=True)\n",
    "airport_df.createOrReplaceTempView(\"airport_table\")\n",
    "orig_airport_df = spark.sql(\"SELECT * from airport_table\")\n",
    "print(\"Original airport count \",orig_airport_df.count())\n",
    "clean_airport_df = spark.sql(\"SELECT * from airport_table where iso_country = 'US' and iata_code is not null\")\n",
    "\n",
    "clean_airport_df.describe\n",
    "print(\"Clean airport count \",clean_airport_df.count())\n",
    "clean_airport_df.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/airports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[City: string, State: string, Median Age: string, Male Population: string, Female Population: string, Total Population: string, Number of Veterans: string, Foreign-born: string, Average Household Size: string, State Code: string, Race: string, Count: string]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# US cities state demographics data\n",
    "us_city_demo_df = spark.read.csv('us-cities-demographics.csv',sep=';',header=True)\n",
    "us_city_demo_df.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State count is  49\n"
     ]
    }
   ],
   "source": [
    "# Write to local parquet first since the file is small and directly writing to S3 is taking forever\n",
    "us_city_demo_df.createOrReplaceTempView(\"us_city_state_table\")\n",
    "us_state_table = spark.sql(\"select 'State Code' as state_cd,State,sum('Total Population') as total_popln,\\\n",
    "                            sum('Foreign-born') as foreign_born_popln\\\n",
    "                            from us_city_state_table\\\n",
    "                             group by 'State Code',State\")\n",
    "print(\"State count is \",us_state_table.count())\n",
    "us_state_table.write.mode(\"overwrite\").parquet(\"state_parquet/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "df_states = spark.read.parquet(\"state_parquet/\")\n",
    "df_states.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/states/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country code count is  288\n"
     ]
    }
   ],
   "source": [
    "# Write countries to S3\n",
    "df_cc = spark.read.csv('countrycodes.csv',header=False,sep='|')\n",
    "print (\"Country code count is \",df_cc.count())\n",
    "df_cc.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/countries/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The Data model consists of the following fact and dimension tables. \n",
    "\n",
    "##### Fact table - i94visitors_fact\n",
    "This fact table contains all the detailed i94 visitor data from the SAS files. The source data is loaded into a staging table. ETL is performed using SQL as data is loaded into the fact table.\n",
    "The fact table contains most of the columns from the source SAS files so no detail is lost. All analytical queries can be run on this fact table. The data model can be enhanced by building \n",
    "summary fact tables to speed up queries if required.\n",
    "\n",
    "##### airports_dim\n",
    "This dimension table is loaded from airport-codes csv file after cleaning up non US airports. The airports dimension provides details about each of the airports in the US and can help disect the fact table by airport\n",
    "\n",
    "##### states_dim\n",
    "The states dimension provides state level demographics such as total population and foreign born population and can provide insights into number of visitors arriving in a state and its foreign born population. \n",
    "\n",
    "##### dates_dim\n",
    "The dates dimension is useful for summarizing the visitors data by arrival dates, month, year, etc. This will provide visibility into how the volume of visitors changes over the course of the year and months within that year.\n",
    "\n",
    "\n",
    "##### countries_dim\n",
    "The Countries dimension lists the countries of origin for the visitors. Data in this dimension can be enahnced further as required. This dimension helps in summarizing the data based on the visitors' countries and can be used to check where people are visiting from. \n",
    "\n",
    "\n",
    "The data model would provide US travel department with a lot of insights such as where people are arriving from and during what season. Although data for all air travellers is not captured here, it would still help various US travel departments to better provision their airport resources and personnel and also help the states involved in performing future capacity planning and upgrades of their airports.  The US embassies in varipus countries can all augment their staff and capacity based on this data.\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# THe data pipeline for the ETL is implemented using Apache airflow\n",
    "# The Dags are available under the dags folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# Data Qualirt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
