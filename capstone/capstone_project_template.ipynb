{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project aims to build a cloud based datawarehouse in Redshift for analysing International visitor statistics by the US National Tourism and Trade Office. They would like to understand the distribution of visitors around all the airports in the US and the seasonal change in number of vistors. This would enable them to assign approriate number of personnel so visitor entry is efficiently handled. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd  \n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read AWS config to get access id\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('dw.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The US Tourism and Trade office would like to understand the following visitor statistics using the I-94 vistor arrival data.\n",
    "\n",
    "1. Visitor arrival by month and year\n",
    "\n",
    "2. Visitors by arrival City and by date\n",
    "\n",
    "3. Visitors by Country and date\n",
    "\n",
    "The proposed solution is to use AWS to build a cloud Data warehouse to anwer their questions. The use of AWS cloud technology enables linear scalabity as the data set grows. AWS also supports a variety of techonologies such as S3, Reshift, EMR and Cassandra suitable for handling massive amounts of data and also to proivde fast response times to end user queries. \n",
    "\n",
    "Initial data analysis is done by using PySpark in the local notebook and rest of the data transformation is handled by Redshift SQL during the ETL stage.\n",
    "\n",
    "The data will be loaded into fact and dimension tables in an Amazon Redshift data warehouse hosted in the cloud. Data is first copied to Amazon S3 service. Using S3 service will enable to scale the data staging area in case huge gigabyte size files are required to be processed. Also, the storage can be scaled back once the processing is done to save cost.\n",
    "\n",
    "Data will be first loaded into staging tables in Amazon Redshift using Copy statements. Following this, data cleansing can be perfomed and data loaded into fact and dimension tables using insert statements. The same SQL queries can be used to create ETL pipelines if required at a later stage.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data\n",
    "\n",
    "The project uses the following Udacity supplied data sets to build the data warehouse.\n",
    "\n",
    "1. I94 immigration SAS data files\n",
    "   This data is provided by the US travel department and is available here - https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "2. Airport data from the supplie airport-codes_csv.csv\n",
    "3. The aiport data is augmented by a manually created airport.csv file from the provided data dictionary.\n",
    "4. Country codes data in csv data format created using I94_SAS_Labels_description dictionary\n",
    "5. State demographics in the supploed us-cities-demographics.csv file\n",
    "6. The State data is augmented using states.txt data in csv format created using I94_SAS_Labels_description dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "i94_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(i94_fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.1\").\\\n",
    "config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']).\\\n",
    "config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']).\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count for ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat is  3157072\n",
      "Record count for ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat is  3096313\n"
     ]
    }
   ],
   "source": [
    "# Read i94 sas data for 2 months - Mar 2016 and Apr 2016 and write to local parquet files\n",
    "sas_files=[\"../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\",\"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\"]\n",
    "for sasfile in sas_files:\n",
    "    df_spark_i94 = spark.read.format('com.github.saurfang.sas.spark').load(sasfile)\n",
    "    print(f\"Record count for {sasfile} is \", df_spark_i94.count())\n",
    "    df_spark_i94.write.mode(\"append\").parquet(\"sas_parquet/\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6253385"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Read parquet data\n",
    "df_all_i94 = spark.read.parquet(\"sas_parquet/\")\n",
    "df_all_i94.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i94 table count: 1100000\n"
     ]
    }
   ],
   "source": [
    "# Clean i94 data\n",
    "## Get distinct values for the columns selected - remove duplicates and limit count to 1.1 million rows.\n",
    "df_all_i94.createOrReplaceTempView(\"visitor_i94_table\")\n",
    "i94_table = spark.sql(\"select distinct cicid, i94yr, i94mon,i94cit,i94port,arrdate,i94mode,i94addr,depdate,i94bir,i94visa,visapost, \\\n",
    "gender,airline,fltno,visatype from visitor_i94_table limit 1100000\")\n",
    "print(\"i94 table count:\",i94_table.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|visapost|gender|airline|fltno|visatype|\n",
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "|  48.0|2016.0|   4.0| 101.0|    NYC|20545.0|    1.0|     NY|20572.0|  68.0|    2.0|     FLR|     M|     AA|00199|      B2|\n",
      "| 270.0|2016.0|   4.0| 103.0|    NYC|20545.0|    1.0|     NY|20560.0|  33.0|    2.0|    null|     M|     LH|00410|      WT|\n",
      "| 424.0|2016.0|   3.0| 103.0|    NYC|20514.0|    1.0|     NY|20574.0|  37.0|    2.0|     VNN|     M|     OS|00087|      B2|\n",
      "| 746.0|2016.0|   4.0| 103.0|    LOS|20545.0|    1.0|   null|20546.0|  12.0|    2.0|    null|  null|     AS|00291|      WT|\n",
      "|1324.0|2016.0|   3.0| 107.0|    LOS|20514.0|    1.0|     CA|20538.0|  33.0|    2.0|    null|  null|     LH|00456|      B2|\n",
      "|1372.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  28.0|    2.0|    null|     F|     SN|01401|      WT|\n",
      "|1392.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20550.0|  16.0|    2.0|    null|     M|     AB|07450|      WT|\n",
      "|1422.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20551.0|  45.0|    2.0|    null|     M|     DL|00047|      WT|\n",
      "|1630.0|2016.0|   4.0| 104.0|    MIA|20545.0|    1.0|     FL|20553.0|  76.0|    2.0|    null|     M|     AB|07000|      WT|\n",
      "|1714.0|2016.0|   4.0| 104.0|    MIA|20545.0|    1.0|     FL|20561.0|  49.0|    2.0|    null|  null|     AA|00039|      WT|\n",
      "|1817.0|2016.0|   3.0| 108.0|    NEW|20514.0|    1.0|     DC|20516.0|  47.0|    1.0|    null|     M|     SK|00909|      WB|\n",
      "|1890.0|2016.0|   3.0| 108.0|    NEW|20514.0|    1.0|     NY|20527.0|  69.0|    2.0|    null|     F|     SK|00909|      WT|\n",
      "|1959.0|2016.0|   4.0| 104.0|    NYC|20545.0|    1.0|     NY|20549.0|  49.0|    2.0|    null|     M|     LX|00022|      WT|\n",
      "|2414.0|2016.0|   3.0| 108.0|    WAS|20514.0|    1.0|     DC|20526.0|  69.0|    2.0|    null|     M|     FI|  645|      WT|\n",
      "|2640.0|2016.0|   3.0| 109.0|    MIA|20514.0|    1.0|     FL|20575.0|  28.0|    2.0|    null|     M|     AA|00039|      WT|\n",
      "|2845.0|2016.0|   4.0| 108.0|    ATL|20545.0|    1.0|     LA|20569.0|  57.0|    2.0|    null|     M|     DL|00071|      WT|\n",
      "|3103.0|2016.0|   4.0| 108.0|    NYC|20545.0|    1.0|     NY|20557.0|  79.0|    2.0|    null|     M|     BA|00113|      WT|\n",
      "|3304.0|2016.0|   4.0| 108.0|    MIA|20545.0|    1.0|   null|20548.0|  52.0|    2.0|    null|     F|     AA|01466|      WT|\n",
      "|3598.0|2016.0|   3.0| 111.0|    NEW|20514.0|    1.0|     FL|20517.0|  34.0|    1.0|    null|     M|     UA|00056|      WB|\n",
      "|3608.0|2016.0|   3.0| 111.0|    NEW|20514.0|    1.0|     NJ|20517.0|  43.0|    1.0|    null|     F|     LX|00018|      WB|\n",
      "+------+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_table.show()\n",
    "#Write i94 parquet to AWS S3 \n",
    "i94_table.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/i94visitors/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original airport count  55075\n",
      "Clean airport count  2133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather data from manually prepared airports file\n",
    "airport_local_df=spark.read.csv('./airport.csv', header=True, sep='|')\n",
    "airport_local_df.createOrReplaceTempView(\"local_airport_table\")\n",
    "\n",
    "#Clean airport data from supplied airports file\n",
    "airport_df = spark.read.csv('./airport-codes_csv.csv', header=True)\n",
    "airport_df.createOrReplaceTempView(\"airport_table\")\n",
    "orig_airport_df = spark.sql(\"SELECT * from airport_table\")\n",
    "print(\"Original airport count \",orig_airport_df.count())\n",
    "\n",
    "# Merge the 2 airport datasets\n",
    "# Augment supplied airports with missing airports from manually created airport list\n",
    "# Assign row number by iata_code using row_number Window function\n",
    "clean_airport_df = spark.sql(\"select a.*, row_number() over(partition by iata_code order by municipality) as rownum\\\n",
    "                               from (SELECT distinct iata_code,ident,type,name,continent,iso_country,iso_region,\\\n",
    "                               municipality,gps_code,local_code,coordinates \\\n",
    "                               from airport_table where iso_country = 'US' and iata_code is not null\\\n",
    "                               union all \\\n",
    "                              select iata_code,null,null,null,null,null,null,municipality,null,null,null from local_airport_table\\\n",
    "                              where iata_code not in (select distinct iata_code from airport_table where iata_code is not null)\\\n",
    "                              ) a\\\n",
    "                              \")\n",
    "\n",
    "clean_airport_df.describe\n",
    "print(\"Clean airport count \",clean_airport_df.count())\n",
    "\n",
    "# Eliminate duplicates by selecting 1st row from every iata_code group\n",
    "clean_airport_df.createOrReplaceTempView(\"clean_airport_table\")\n",
    "nondup_airport_df = spark.sql(\"select iata_code,ident,type,name,continent,iso_country,iso_region,\\\n",
    "                               municipality,gps_code,local_code,coordinates from clean_airport_table \\\n",
    "                                where rownum=1\")\n",
    "nondup_airport_df.count()\n",
    "nondup_airport_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------+--------------------+---------+-----------+----------+----------------+--------+----------+--------------------+\n",
      "|iata_code|ident|          type|                name|continent|iso_country|iso_region|    municipality|gps_code|local_code|         coordinates|\n",
      "+---------+-----+--------------+--------------------+---------+-----------+----------+----------------+--------+----------+--------------------+\n",
      "|      ARC| PARC|medium_airport|Arctic Village Ai...|       NA|         US|     US-AK|  Arctic Village|    PARC|       ARC|-145.578995, 68.1147|\n",
      "|      BKD| KBKD| small_airport|Stephens County A...|       NA|         US|     US-TX|    Breckenridge|    KBKD|       BKD|-98.8909988403000...|\n",
      "|      CHZ| K2S7| small_airport|Chiloquin State A...|       NA|         US|     US-OR|       Chiloquin|    K2S7|       2S7|-121.879062653, 4...|\n",
      "|      CIU| KCIU|medium_airport|Chippewa County I...|       NA|         US|     US-MI| Sault Ste Marie|    KCIU|       CIU|-84.4723968505859...|\n",
      "|      CTO| K3C8| small_airport|Calverton Executi...|       NA|         US|     US-NY|       Calverton|     3C8|       3C8|-72.7919006348, 4...|\n",
      "|      GLE| KGLE| small_airport|Gainesville Munic...|       NA|         US|     US-TX|     Gainesville|    KGLE|       GLE|-97.1969985962, 3...|\n",
      "|      GNF| K2O1| small_airport|       Gansner Field|       NA|         US|     US-CA|          Quincy|    null|       2O1| -120.945, 39.943902|\n",
      "|      GUP| KGUP|medium_airport|Gallup Municipal ...|       NA|         US|     US-NM|          Gallup|    KGUP|       GUP|-108.789001465, 3...|\n",
      "|      HLN| KHLN|medium_airport|Helena Regional A...|       NA|         US|     US-MT|          Helena|    KHLN|       HLN|-111.983001708984...|\n",
      "|      ISM| KISM|medium_airport|Kissimmee Gateway...|       NA|         US|     US-FL|         Orlando|    KISM|       ISM|-81.4371032715, 2...|\n",
      "|      ISW| KISW| small_airport|Alexander Field S...|       NA|         US|     US-WI|Wisconsin Rapids|    KISW|       ISW|-89.8389968872000...|\n",
      "|      LNY| PHNY|medium_airport|       Lanai Airport|       NA|         US|     US-HI|      Lanai City|    PHNY|       LNY|-156.951004028320...|\n",
      "|      MUE| PHMU|medium_airport|Waimea Kohala Air...|       NA|         US|     US-HI|         Kamuela|    PHMU|       MUE|-155.667999267578...|\n",
      "|      OBE| KOBE| small_airport|Okeechobee County...|       NA|         US|     US-FL|      Okeechobee|    KOBE|       OBE|-80.8498001099, 2...|\n",
      "|      OEO| KOEO| small_airport|L O Simenstad Mun...|       NA|         US|     US-WI|         Osceola|    KOEO|       OEO|-92.691902160645,...|\n",
      "|      OWD| KOWD|medium_airport|Norwood Memorial ...|       NA|         US|     US-MA|         Norwood|    KOWD|       OWD|-71.1728973389, 4...|\n",
      "|      PWT| KPWT|medium_airport|Bremerton Nationa...|       NA|         US|     US-WA|       Bremerton|    KPWT|       PWT|-122.76499938965,...|\n",
      "|      RSH| PARS| small_airport|Russian Mission A...|       NA|         US|     US-AK| Russian Mission|    PARS|       RSH|-161.319458008, 6...|\n",
      "|      SVC| KSVC| small_airport|Grant County Airport|       NA|         US|     US-NM|     Silver City|    KSVC|       SVC|-108.155998229980...|\n",
      "|      WRI| KWRI|medium_airport|Mc Guire Air Forc...|       NA|         US|     US-NJ|     Wrightstown|    KWRI|       WRI|-74.59169769, 40....|\n",
      "+---------+-----+--------------+--------------------+---------+-----------+----------+----------------+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write to local parquet first since the file is small and directly writing to S3 is taking forever\n",
    "nondup_airport_df.write.mode(\"overwrite\").parquet(\"airports/\")\n",
    "\n",
    "# Read from parquet\n",
    "df_airports = spark.read.parquet(\"airports/\")\n",
    "\n",
    "#Write Airports to S3\n",
    "df_airports.describe\n",
    "df_airports.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/airports/\")\n",
    "df_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|state_code|            state|\n",
      "+----------+-----------------+\n",
      "|        AL|          ALABAMA|\n",
      "|        AK|           ALASKA|\n",
      "|        AZ|          ARIZONA|\n",
      "|        AR|         ARKANSAS|\n",
      "|        CA|       CALIFORNIA|\n",
      "|        CO|         COLORADO|\n",
      "|        CT|      CONNECTICUT|\n",
      "|        DE|         DELAWARE|\n",
      "|        DC|DIST. OF COLUMBIA|\n",
      "|        FL|          FLORIDA|\n",
      "|        GA|          GEORGIA|\n",
      "|        GU|             GUAM|\n",
      "|        HI|           HAWAII|\n",
      "|        ID|            IDAHO|\n",
      "|        IL|         ILLINOIS|\n",
      "|        IN|          INDIANA|\n",
      "|        IA|             IOWA|\n",
      "|        KS|           KANSAS|\n",
      "|        KY|         KENTUCKY|\n",
      "|        LA|        LOUISIANA|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read locally prepared state file\n",
    "local_state_df = spark.read.csv('states.txt',header=True)\n",
    "local_state_df.createOrReplaceTempView(\"local_state_table\")\n",
    "local_state_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State count is  55\n",
      "+--------+--------------------+------------+------------------+\n",
      "|state_cd|               State| total_popln|foreign_born_popln|\n",
      "+--------+--------------------+------------+------------------+\n",
      "|      MT|             Montana|    906470.0|           29885.0|\n",
      "|      NC|      North Carolina| 1.5300995E7|         1896635.0|\n",
      "|      MD|            Maryland|   6560645.0|         1148970.0|\n",
      "|      CO|            Colorado| 1.4678345E7|         1688155.0|\n",
      "|      CT|         Connecticut|   4355096.0|         1114250.0|\n",
      "|      IL|            Illinois|  2.251439E7|         4632600.0|\n",
      "|      NJ|          New Jersey|   6931024.0|         2327750.0|\n",
      "|      DE|            Delaware|    359785.0|           16680.0|\n",
      "|      DC|District of Columbia|   3361140.0|          475585.0|\n",
      "|      AR|            Arkansas|   2882889.0|          307753.0|\n",
      "|      TN|           Tennessee| 1.0690165E7|          900149.0|\n",
      "|      LA|           Louisiana|   6502975.0|          417095.0|\n",
      "|      AK|              Alaska|   1493475.0|          166290.0|\n",
      "|      CA|          California|1.23444353E8|       3.7059662E7|\n",
      "|      NM|          New Mexico|   4195210.0|          445560.0|\n",
      "|      UT|                Utah|   5119677.0|          651811.0|\n",
      "|      MI|            Michigan| 1.0885238E7|         1214547.0|\n",
      "|      NY|            New York| 4.9002055E7|       1.7186873E7|\n",
      "|      NH|       New Hampshire|    990990.0|          135995.0|\n",
      "|      WA|          Washington| 1.2500535E7|         2204810.0|\n",
      "+--------+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# US cities state demographics data\n",
    "# Augment with data from manually preparted states file - add only missing states\n",
    "\n",
    "us_city_demo_df = spark.read.csv('us-cities-demographics.csv',sep=';',header=True)\n",
    "us_city_demo_df.describe\n",
    "us_city_demo_df.createOrReplaceTempView(\"us_city_state_table\")\n",
    "us_state_table = spark.sql(\"select `State Code` as state_cd,State,sum(`Total Population`) as total_popln,\\\n",
    "                            sum(`Foreign-born`) as foreign_born_popln\\\n",
    "                           from us_city_state_table\\\n",
    "                           group by `State Code`,State\\\n",
    "                           union all\\\n",
    "                           select state_code,state,null,null\\\n",
    "                           from local_state_table \\\n",
    "                           where state_code not in (select distinct `State Code` from us_city_state_table)\\\n",
    "                           \")\n",
    "print(\"State count is \",us_state_table.count())\n",
    "us_state_table.show()\n",
    "\n",
    "# Write to local parquet first since the file is small and directly writing to S3 is taking forever\n",
    "us_state_table.write.mode(\"overwrite\").parquet(\"state_parquet/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Upload States data to S3\n",
    "df_states = spark.read.parquet(\"state_parquet/\")\n",
    "df_states.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/states/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country code count is  288\n"
     ]
    }
   ],
   "source": [
    "# Write countries to local parquet first\n",
    "df_cc = spark.read.csv('countrycodes.csv',header=False,sep='|')\n",
    "print (\"Country code count is \",df_cc.count())\n",
    "df_cc.write.mode(\"overwrite\").parquet(\"country_parquet/\")\n",
    "\n",
    "# Upload countries data to S3\n",
    "df_countries = spark.read.parquet(\"country_parquet/\")\n",
    "df_countries.write.mode(\"overwrite\").parquet(\"s3a://ctsprojbucket/countries/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The Data model consists of the following fact and dimension tables. \n",
    "\n",
    "##### Fact table - i94visitors_fact\n",
    "This fact table contains all the detailed i94 visitor data from the SAS files. The source data is loaded into a staging table. ETL is performed using SQL as data is loaded into the fact table.\n",
    "The fact table contains most of the columns from the source SAS files so no detail is lost. All analytical queries can be run on this fact table. The data model can be enhanced by building \n",
    "summary fact tables to speed up queries if required.\n",
    "\n",
    "##### airports_dim\n",
    "This dimension table is loaded from airport-codes csv file after cleaning up non US airports. The airports dimension provides details about each of the airports in the US and can help disect the fact table by airport\n",
    "\n",
    "##### states_dim\n",
    "The states dimension provides state level demographics such as total population and foreign born population and can provide insights into number of visitors arriving in a state and its foreign born population. \n",
    "\n",
    "##### dates_dim\n",
    "The dates dimension is useful for summarizing the visitors data by arrival dates, month, year, etc. This will provide visibility into how the volume of visitors changes over the course of the year and months within that year.\n",
    "\n",
    "\n",
    "##### countries_dim\n",
    "The Countries dimension lists the countries of origin for the visitors. Data in this dimension can be enahnced further as required. This dimension helps in summarizing the data based on the visitors' countries and can be used to check where people are visiting from. \n",
    "\n",
    "\n",
    "The data model would provide US travel department with a lot of insights such as where people are arriving from and during what season. Although data for all air travellers is not captured here, it would still help various US travel departments to better provision their airport resources and personnel and also help the states involved in performing future capacity planning and upgrades of their airports.  The US embassies in various countries can all augment their staff and capacity based on this data.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data pipeline is described briefly below\n",
    "1. Data is first staged in Amazon S3 in parquet format. \n",
    "2. The data is then loaded into fact and dimension tables in Amazon Redshift for analysis by end users. \n",
    "3. Apache airflow is used to build the data pipelne. \n",
    "4. The stage to Redshift operator is reused while loading data eliminating redundant code. \n",
    "5. Any ETL while loading data is performed by SQL queries. \n",
    "6. The airflow pipeline is called capstone dag and is available under the project submission folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# THe data pipeline for the ETL is implemented using Apache airflow\n",
    "# The Dag called capstone is available under the dags folder. \n",
    "# https://github.com/learnds/DataEngineer/blob/master/capstone/dags/capstone.py\n",
    "# The table creation script is available in create_tables.sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Perform quality checks here\n",
    "# Data quality checks are performed in the Airflow Dag\n",
    "1. The fact table - i94visitors_fact is checked for null values for 'reasonforvisit' column. A threshold can be set to fail the Dag if the count of null/missing values exceeds the execpted count.\n",
    "2. The fact table - i94visitors_fact is checked for missing dimension values by querying for '-1' in airportid or stateid columns. A threshold value of 10000 is set. If the counts exceed this threshold, the dag fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "There are 4 dimension tables and 1 fact table\n",
    "\n",
    "##### Dimension tables\n",
    "\n",
    "##### dates_dim\n",
    "The dates_dim dimension table contains date related information for querying by date, day, month, year, etc.\n",
    "date - primary key date column used as the primary key\n",
    "day - day of the week\n",
    "week - week of the year\n",
    "month - month of the year\n",
    "year - 4 digit year\n",
    "\n",
    "##### states_dim\n",
    "This table contains state level demographics for 50 US states along with Washington DC, Virgin Islands and a few other territories around the US.\n",
    "The table may be useful in determining if there is a \"causal\" relationship between the nationality of the pepple arriving at a particular airport and the nationality of the immigrants living in that state. \n",
    "\n",
    "stateid - Primary key, 2 letter state id for the various states in the US and surrounding independent areas\n",
    "statename - Name of the state\n",
    "totalpopulation - Total population of the state\n",
    "foreignborn - Total foreign born populaton \n",
    "\n",
    "##### airports_dim\n",
    "This table contains valueable information about various airports in the US. This will be useful in charting traffic across various airports in the country\n",
    "\n",
    "airportid - Primary key, Airport Id usually consists of 3 letters used by the airline industry\n",
    "identifier - internal identification number of the airport\n",
    "type - type and size of the airport, usually small, medium, heliport, etc\n",
    "name - Name of the airport\n",
    "continent - Redundant column indicating continent\n",
    "isocountry - Redudant column, only stores US in this exercise\n",
    "isoregion - Region of the airport usually derived from the 2 letter State \n",
    "municipality - City/Town where the airport is located\n",
    "gpscode - GPS code for the airport\n",
    "localcode - Local code for the airport\n",
    "coordinates - Latitiude/Longitude coordinates for the airport\n",
    "\n",
    "##### countries_dim\n",
    "This contains the country codes and correspoding names of the countries people arrive from. It is vert useful in understanding traffic volume from various countries.\n",
    "The dimension can be expanded to include other additional columns.\n",
    "\n",
    "countryid - Primary key, Numerical country id used in i94 visitors souce data\n",
    "countryname - name of the country the person arrived from\n",
    "\n",
    "##### i94visitors_fact\n",
    "\n",
    "This is a detailed Fact table that stores visitor level information for each traveller visiting the US.\n",
    "\n",
    "visitorid - The unique visitor id derived by applying md5 to the i94 cicid raw column from the dataset\n",
    "arrivaldate - The arrivaldate of the visitor derived from the SAS date. It provides complete date information including day of the month.\n",
    "airportid - The airport id where the visitor arrived. Stores '-1' for invalid airports\n",
    "stateid - The state id where the visitor arrived. Stores '-1' for invalid or missing states\n",
    "visitorcountryid - The id corresponding the country of the visitor\n",
    "i94date - Partial i94 date rounded to the 1st of the month\n",
    "arrivalmode - mode of arrival of the visitor - land, air, sea, etc\n",
    "reasonforvisit - Reason for the traveler's visit - Business, pleasure, etc\n",
    "departuredate - the date the visitor left the country\n",
    "visitorage - Age of the visitor in years\n",
    "visatype - Visa type used by the visitor - Visitor, business visa, etc\n",
    "visaissuedloc - Location in the visitors country where the visa was issued.\n",
    "gender - gender of the visitor\n",
    "airline - The airline used by the visitor\n",
    "fltno - The flight number of the flight on which the visitor arrived.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "A cloud based solution such as AWS provides high scalability and high availability. Nodes can be added and removed dynamically, storage can be expanded with virtually as much storage as you afford.\n",
    "Data is initially staged in Amazon S3 which provides high performance storage and supports parquet format to handle massive amounts of data.\n",
    "Redshift is a very popular AWS database for hosting Data Warehouses. It is highly scalable and a clusted can be spun up in minutes. It provides many ways to partition data as well. Since it is directly licensed by Amazon, no additional licensing costs are incurred apart from the usage fee.\n",
    "The ETL pipeline is handled by Apache Airflow that is a fairly mature Opensource tool for handling DAGs. It can seamlessly connect to AWS to run ETL at various schedules.\n",
    "\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Data as in the supplied dataset can be loaded on a monthly basis. The SAS files are partitioned by month and are probably available at the beginning of the following month. Initial analysis shows that the monthly volume is around 3 million rows which is pretty small and is easily handled by a 2 node Redshift cluster.\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " If the data set increases by 100x, that would amount to 300 million rows per month. While this can be easily ingested into a 16 node (or larger) high performance SSD Redshift cluster, resources may not see full utilization after the load is complete. It is best to receive smaller SAS datasets on a daily basis that would amount to 10 million rows and can be efficiently loaded using a 8 node cluster.\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " The SLA here is load the dashboard data by 7 am. The Airflow DAG can be set up with a 'sla' parameter setting to ensure loads complete by 6 am to provide some buffer. A 'sla_miss_callback' function can be implemented in the DAG to alery oncall personnel in case the loads do not complete on time. \n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " This is a read heavy scenario. \n",
    " A 16 or 32 node Redshift cluster can be created to support 100's of users.\n",
    " Depending on the queries, additional summary tables also called cubes can be created as part of the ETL to support summarization queries. \n",
    " Smaller  tables can use the distribution feature and larger tables can use the sorted key feature to store data in the sorted order to  improve query performance for useds.\n",
    " Redshift materialized views can also be created to support such summarizations transparaently. \n",
    " If the users need to query detailed data, one or more Cassandra tables can be created based on the user queries on appropriately sized cluster to provide response times of less than a few seconds or less than a second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
