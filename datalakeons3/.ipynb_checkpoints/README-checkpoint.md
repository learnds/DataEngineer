# Sparkify Data Lake

## Summary

This is the Sparkify Data Lake built in the Cloud using Amazon S3 service. It's main goal is to support the Analytics team in analyzing user song listening preferences. Both the source data used for loading the tables and the target data lake tables are available in Amazon S3. 
The ETL pipeline reads data from S3 and processes them into 5 tables in Star schema format using Spark and loads them back into S3 for further analysis.

### DataSet Description  

#### Song Dataset  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song

#### Log Dataset  
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### Schema Design
The Dimension tables are songs, users, artists and time. The fact table is called songplays. The time table is partitioned by year and month.  The songplays fact table is also partitioned by year and month. This enables quick response times for time related queries.

### ETL Pipeline
Data processing in ETL is mainly handled using Spark SQL. The staging tables are created from data frames using Spark SQL and stored in Parquet format in S3 to optimize space and to improve read performance during analysis. 

### Preliminary Data Analysis
There are 14896 rows loaded into staging_songs table. 8056 rows are loaded into staging_events table. 

The DW tables contain the following number of rows:
1. songs -  14896
2. users - 104
3. artists - 10025
4. time - 6813
5. songplays - 822


### 
### Runtime instructions

The Data Lake can be created by running the etl.py script. To create the data lake, first create an Amazon EMR cluster, update the dl.cfg config file with connection key and secret and run the following command:  

python etl.py  -- for running  locally

To run the job on Amazon EMR, run the following on the EMR master host:

/usr/bin/spark-submit --master yarn ./etl.py | tee -a etl.log

This loads S3 source data into a Star schema in parquet format files hosted in S3. 

### Files included in the project

README.md - this markup file  

test_etl.ipynb - test python jupyter notebook for testing Spark SQL statements, file read and write statements

etl.py - Python script that runs the ETL to load the tables into the Data Lake on S3  
